<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data) | 喜欢小杜</title><meta name="author" content="小杜老公"><meta name="copyright" content="小杜老公"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data) 开山之作: Communication-Efficient Learning of Deep Networks from Decentralized Data ​ 现代移动设备拥有大量的适合模型学习的数据，基于这些数据训">
<meta property="og:type" content="article">
<meta property="og:title" content="联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data)">
<meta property="og:url" content="http://xinyudu.cn/2023/11/30/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-01/index.html">
<meta property="og:site_name" content="喜欢小杜">
<meta property="og:description" content="联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data) 开山之作: Communication-Efficient Learning of Deep Networks from Decentralized Data ​ 现代移动设备拥有大量的适合模型学习的数据，基于这些数据训">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xinyudu.cn/img/default_cover.jpg">
<meta property="article:published_time" content="2023-11-30T09:32:29.000Z">
<meta property="article:modified_time" content="2023-11-30T13:19:35.604Z">
<meta property="article:author" content="小杜老公">
<meta property="article:tag" content="毕业设计">
<meta property="article:tag" content="小刘的">
<meta property="article:tag" content="联邦学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://xinyudu.cn/img/default_cover.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://xinyudu.cn/2023/11/30/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-01/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":600},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 小杜老公","link":"链接: ","source":"来源: 喜欢小杜","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-30 21:19:35'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mystyle.css"><link rel="stylesheet" href="/css/font.css"><meta name="generator" content="Hexo 7.0.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/head.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Agreement/"><i class="fa-fw fa-solid fa-handshake"></i><span> 约定</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/default_cover.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="喜欢小杜"><img class="site-icon" src="/img/nav.png"/><span class="site-name">喜欢小杜</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Agreement/"><i class="fa-fw fa-solid fa-handshake"></i><span> 约定</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-30T09:32:29.000Z" title="发表于 2023-11-30 17:32:29">2023-11-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-11-30T13:19:35.604Z" title="更新于 2023-11-30 21:19:35">2023-11-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/">毕业设计</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1
id="联邦学习入门-01communication-efficient-learning-of-deep-networks-from-decentralized-data">联邦学习入门-01(Communication-Efficient
Learning of Deep Networks from Decentralized Data)</h1>
<p>开山之作: <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1602.05629.pdf">Communication-Efficient
Learning of Deep Networks from Decentralized Data</a></p>
<p>​
现代移动设备拥有大量的适合模型学习的数据，基于这些数据训练得到的模型可以极大地提升用户体验。例如，语言模型能提升语音设别的准确率和文本输入的效率，图像模型能自动筛选好的照片。然而，移动设备拥有的丰富的数据经常具有关于用户的敏感的隐私信息且多个移动设备所存储的数据总量很大，这样一来，不适合将各个移动设备的数据上传到数据中心，然后使用传统的方法进行模型训练。作者提出了一个替代方法，这种方法可以基于分布在各个设备上的数据（无需上传到数据中心），然后通过局部计算的更新值进行聚合来学习到一个共享模型。作者定义这种非中心化方法为“联邦学习”。作者针对深度网络的联邦学习任务提出了一种实用方法，这种方法在学习过程中多次对模型进行平均。同时，作者使用了五种不同的模型和四个数据集对这种方法进行了实验验证。实验结果表明，这种方法面对不平衡以及非独立同分布的数据，具有较好的鲁棒性。在这种方法中，通信所产生的资源开销是主要的瓶颈，实验结果表明，与同步随机梯度下降相比，该方法的通信轮次减少了10-100倍。</p>
<h2
id="联邦学习联邦学习有如下的属性">联邦学习：联邦学习有如下的属性</h2>
<p>​
从多个移动设备中存储的真实数据中进行模型训练比从存储在数据中心的数据中进行模型训练更具优势；
由于数据具有隐私，且多个移动设备所存储的数据总量很大，因此不适合将其上传至数据中心再进行模型训练；
对于监督学习任务，数据中的标签信息可以从用户与应用程序的交互中推断出来。</p>
<h2 id="隐私">隐私</h2>
<p>​
相比于中心化数据的训练方法，联邦学习方法在隐私方面具有很大的优势。即使将数据进行匿名化之后，如果联合其他数据依然有可能泄露用户隐私[37]。相反，联邦学习过程中的信息传输是为了改进一个模型的最小的必要信息（隐私保护的强度依赖于更新值）^1。这些更新值本身是“短暂的”，即它们所包含的信息量不高于原始数据所含的信息量（通过数据处理不等式说明），并且一般是更新值包含很少的信息量。由于聚合算法不需要关于更新值的来源信息，所以，不需要通过混合网络（例如Tor）来识别元数据或基于一个可信的第三方就可以传输更新值。在本文末尾，作者简要讨论了一下将联邦学习与多方安全计算和差分隐私进行融合的可能性。</p>
<h2 id="联邦优化">联邦优化</h2>
<p>​
作者将联邦学习中隐含的优化问题称为联邦优化，其与分布式优化有着密切的联系。较之于典型的分布式优化问题，联邦优化有很多不同的地方：</p>
<ul>
<li><p><strong>非独立同分布</strong>：一个客户端存储的训练数据主要是基于一个特定用户在使用该移动设备的过程中产生的，因此，任何一个用户的本地数据集都不能表示整体分布。</p></li>
<li><p><strong>非平衡</strong>：类似地，一些用户可能对某项服务或某个应用软件使用比其他用户频繁，则在该客户端上将产生大量的训练数据。</p></li>
<li><p><strong>大规模分布</strong>：作者预计参与优化的客户端数量将远远大于所有客户端所拥有的数据量总量的平均数。即：若设有<span
class="math inline">\(D\)</span>个客户端参与优化，每个客户端的数据量分布为<span
class="math inline">\(N_i, i=1,2,...,D\)</span>，则有 <span
class="math inline">\(D \gg
\frac{1}{D}\sum_{i=1}^D{N_i}\)</span>。</p></li>
<li><p><strong>通信受限</strong>：移动设备具有经常掉线、传输速率低以及通信成本高的特点</p></li>
</ul>
<p>​
本文，作者重点关注优化任务中的非独立同分布和不平衡的问题，以及通信受限的临界属性。一个可以部署的联邦优化系统必须能够解决很多问题：</p>
<ol type="1">
<li>由于数据被添加或删除导致客户端数据集发生了变动；</li>
<li>客户端的可用性与其存储的数据分布具有复杂的关系（例如，说美式英语的人使用的手机与说英式英语的手机，这两个客户端具有可用性的时间段不一样）；</li>
<li>可能会存在从来不响应的客户端或着发送已损毁的更新值的客户端；</li>
</ol>
<p>​
这些问题超出了本文的研究范围。作者在一个人为控制的实验环境下进行实验，但是这种实验环境仍然存在客户端可用性和数据非平衡以及非独立同分布的关键问题。作者假设一个同步更新框架，通过多轮通信进行更新。现有固定的<span
class="math inline">\(K\)</span>个客户端，每个客户端拥有一个固定的本地数据集。每轮更新的开始，随机选择<span
class="math inline">\(C\)</span>个客户端，并且服务器给每个客户端发送现有的全局算法状态（例如算法状态可以是当前全局模型的参数值）。在每轮学习过程中仅选用一部分客户端是为了提升性能，因为实验表明，<strong>当客户端超过一定数量后学习性能会下降。每一个被选择的客户端基于本地存储的数据以及全局状态进行计算更新，然后将更新后的算法状态发送给服务器。服务器利用客户端发送回来的算法状态对全局状态进行更新，并且重复这个过程。</strong></p>
<p>​
虽然作者关注于非凸的神经网络目标函数，但作者认为该算法适用于以下形式的目标函数，该目标函数为任何有限个目标函数的和：
<span class="math display">\[
\begin{align}
\min_{w\in{\mathbb{R}^{d}}}f(w) \qquad wh &amp; ere\quad
f(w)\overset{def}{=} \frac{1}{n}\sum^n_{i=1}f_i(w)
\end{align}
\]</span> 对于一个机器学习问题，设<span
class="math inline">\(f_i(w)=loss(x_i,y_i;w)\)</span>则其表示给定模型参数<span
class="math inline">\(w\)</span>的条件下，关于第<span
class="math inline">\(i\)</span>个样本<span class="math inline">\((x_i,
y_i)\)</span>的损失。我们假设有<span
class="math inline">\(K\)</span>个客户端,<span
class="math inline">\(\mathcal{P}_k\)</span>表示第<span
class="math inline">\(k\)</span>个客户端的数据点的索引集，<span
class="math inline">\(n_k = |\mathcal{P}_k|\)</span>为第<span
class="math inline">\(k\)</span>个客户端所拥有的数据量。上述目标函数（1）可以重新写为：
<span class="math display">\[
\begin{align}
f(w)=\sum^{K}_{k=1} \frac{n_k}{n}F_k(w) \qquad wh &amp; ere\qquad
F_k(w)=\frac{1}{n_k}\sum_{i \in \mathcal{p_k}}f_i(w)
\end{align}
\]</span> 其中 <span class="math inline">\(n =
\sum^{K}_{k=1}n_k\)</span>'</p>
<p>​ 如果将全部数据集随机均匀地分配到每个客户端中，即所有的<span
class="math inline">\(\mathcal{P}_k\)</span>均为独立同分布的数据集，则有
<span class="math inline">\(\mathbb{E}_{p_k}[F_k(w)] =
f(w)\)</span>，等式左边表示的是关于分配给每个客户端的数据上的平均损失的期望。这是分布式优化算法采用的典型的独立同分布假设；作者考虑的是不满足独立同分布假设的情况（即,<span
class="math inline">\(F_k\)</span>是一个对<span
class="math inline">\(f\)</span>任意的糟糕的近似）。</p>
<p>​
在数据中心存储的优化中，通信开销相对较小，计算开销占主导地位，最近很多研究工作强调使用GPU可以降低计算开销。相反，在联邦优化中，通信开销占主导地位—作者将上传带宽限制为1MB/s或更少。此外，客户段通常只有在充电、接通电源和未计费的Wi-Fi连接时才会参与优化。此外，作者希望每个客户端每天仅仅参与一小部分轮次的训练。另一方面，因为相比于全部数据，任何一个单一设备所具有的数据量较少，且现代手机有相对快的处理器（包括GPU），所以对于很多类型的模型，较之于通信开销，计算不是一个主要问题。因此，本文目的是使用额外的计算来减少训练模型所需通信的轮次。有两种基本的额外计算方法：</p>
<ol type="1">
<li>提高并行度，每两轮通信间，使用更多的独立工作的客户端；</li>
<li>增加每个客户端的计算：每个客户端除了执行一个简单的计算（比如计算梯度），还要在每两轮通信间进行更复杂的计算；</li>
</ol>
<p>​
作者研究了这两种方法，但是一旦使用了最低级别的客户端并行性，则主要通过在每个客户端上添加了更多的计算来实验加速（这里的加速指的是减少了通信轮次）。</p>
<p>​ <strong>相关工作</strong>：McDonald等人<a href="#fn1"
class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>研究了通过迭代平均本地训练的模型来对感知机进行分布式训练，Povey等人<a
href="#fn2" class="footnote-ref" id="fnref2"
role="doc-noteref"><sup>2</sup></a>研究了语音识别深度神经网络的分布式训练，Zhang等人<a
href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a>研究了使用“软”平均的异步训练方法。这些研究工作仅考虑了数据中心化背景（最多有16个工作节点，以及使用了快速传输网络）下的任务，并没有考虑具有数据不平衡且非独立同分布特点的联邦学习任务。作者使用这种风格（即迭代平均本地训练的模型）的算法来解决联邦学习问题，并且进行了适当的实验评估，基于这样的研究，提出与数据中心化设置中不同的问题，并且在联邦学习中这种算法需要结合不同的方法。</p>
<p>​ 与本文的研究动机相似，Neverova等人<a href="#fn4"
class="footnote-ref" id="fnref4"
role="doc-noteref"><sup>4</sup></a>也讨论了保护设备中的用户数据的隐私的优点。Shokri和Shmatikov的研究工作与本文研究工作有一些相似之处：他们关注于训练深度网络，<strong>强调隐私的重要性以及通过在每一轮通信中仅共享一部分参数，进而降低通信开销</strong>；然而，他们也没有考虑数据的不平衡以及非独立同分布性，并且他们的研究工作缺乏实验评估。</p>
<p>​
在“凸”背景下，分布式优化和评估的问题被广泛研究，并且一些算法特别关注了通信效率。除了凸假设，已有的研究研究工作一般要求客户端数量远小于每个客户端的样本数量，这样在各个客户端之间数据（每个客户端的数据量相同）是独立同分布的。这些假设在联邦优化中均不成立。分布式随机梯度下降的异步形式也被应用于神经网络的训练中，例如：Dean等人<a
href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>的研究工作，但是这些方法要求在联邦学习中进行大量的更新。分布式共识算法[41]弱化了独立同分布假设，但是仍然没有解决在通信受限的条件下，大量客户端进行联邦学习的问题。</p>
<p>​
作者认为（参数化）算法就是简单的one-shot（一次性）平均，其中每个客户端使得模型在本地数据集上的损失最小，通过平均这些模型来产生最终的全局模型。这种方法已经在凸情况以及数据独立同分布背景下被广泛研究，并且最坏的情况是，全局模型没有比单一客户端训练的模型更好。</p>
<h2 id="联邦平均算法">联邦平均算法</h2>
<p>​
最近深度学习取得巨大成功在很大程度上依赖于随机梯度下降优化算法及其变种；事实上，在很多应用方面的进步可以理解为，采用了易于使用梯度下降进行优化的模型结构（以及损失函数）<a
href="#fn6" class="footnote-ref" id="fnref6"
role="doc-noteref"><sup>6</sup></a>。因此，作者在构建联邦优化算法时自然地使用了随机梯度下降。</p>
<p>​
随机梯度下降天然地可以被用于联邦优化，因为，每轮通信完成一次基于一个批次数据的梯度计算（在被随机选择的客户端上进行）。这种方法具有很高的计算效率，但是若想学习出好的模型，需要大量轮次的训练（甚至即使使用了一个如批次归一化的高级方法，Ioffe和Szegedy<a
href="#fn7" class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a>构建手写数字识别的模型训练了50000轮，每次训练使用60个样本），作者在CIFAR-10数据集上的实验考虑了该基准（即训练出好的模型所需要的训练次数）。</p>
<p>​
在联邦框架下，更多用户的参与几乎不会增加时间消耗，所以作者使用大批量同步随机梯度下降法做为对比的基准，因为Chen等人[8]的实验验证了该方法在数据中心化学习框架下可以达到很好的效果，且优于异步方法。为了在联邦学习中应用该方法，作者每轮选择<span
class="math inline">\(C\)</span>比例的客户端，在这些客户端上基于全部数据计算损失函数的梯度值。因此，<span
class="math inline">\(C\)</span>控制了全局批次大小，如果<span
class="math inline">\(C=1\)</span>，那么意味着全批次梯度下降。作者称这种基线算法为联邦随机梯度下降。</p>
<p>参数更新方式有两种, 一种典型的联邦随机梯度下降设置<span
class="math inline">\(c=1\)</span> 以及一个固定的学习率<span
class="math inline">\(\eta\)</span>：</p>
<ol type="1">
<li><strong>典型的联邦随机梯度下降</strong>第 <span
class="math inline">\(k\)</span>个客户端计算梯度为 <span
class="math inline">\(g_k = \bigtriangledown
F_k(w_t)\)</span>，中心服务器聚合每个客户端计算的梯度以此来更新模型参数，即：</li>
</ol>
<p><span class="math display">\[
\begin{align}
w_{t+1} \gets w_t - \eta\sum_{k=1}^K\frac{n_k}{n}g_k = w_t -
\eta\bigtriangledown f(w_t)
\end{align}
\]</span></p>
<p>其中，$ _{k=1}^Kg_k = f(w_t)$</p>
<pre><code>2. **另一种等价更新方法**为，每个客户端给予本地数据分别各自对当前模型参数 $w_t$进行更新，即：</code></pre>
<p><span class="math display">\[
\begin{align}
w^k_{t+1} \gets w_t - \eta g_k
\end{align}
\]</span></p>
<p>然后中心服务器对每个客户端更新后的参数进行加权平均： <span
class="math display">\[
\begin{align}
w_{t+1} \gets \sum^{K}_{k=1}\frac{n_k}{n}w^k_{t+1}
\end{align}
\]</span>
按照第二种参数更新方法，每个客户端可以独立地更新模型参数多次，然后再将更新好的参数发送给中心服务器进行加权平均。</p>
<p>作者称这种方法为<strong>联邦平均（FedAvg）</strong>。算法的计算量与三个参数有关：</p>
<ol type="1">
<li><span
class="math inline">\(C\)</span>：每轮训练选择客户端的比例；</li>
<li><span
class="math inline">\(E\)</span>：每个客户端更新参数的循环次数所设计的一个因子；</li>
<li><span
class="math inline">\(B\)</span>：客户端更新参数时，每次梯度下降所使用的数据量；</li>
</ol>
<figure>
<img
src="https://pic1.zhimg.com/80/v2-5108ab37b6110d1d962f10590a82a720_720w.webp"
alt="img" />
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p><strong><em>对于一般的非凸目标函数，在参数空间进行模型平均将得到一个较差的模型</em></strong>。</p>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Ryan McDonald, Keith Hall, and Gideon Mann. Distributed
training strategies for the structured perceptron. In NAACL HLT, 2010.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Daniel Povey, Xiaohui Zhang, and Sanjeev
Khudanpur.Parallel training of deep neural networks with natural
gradient and parameter averaging. In ICLR Workshop Track, 2015.<a
href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Sixin Zhang, Anna E Choromanska, and Yann LeCun.Deep
learning with elastic averaging sgd. In NIPS. 2015.<a href="#fnref3"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Natalia Neverova, Christian Wolf, Griffin Lacey, Lex
Fridman, Deepak Chandra, Brandon Barbello, and Graham W. Taylor.
Learning human identity from motion patterns. IEEE Access, 4:1810–1820,
2016.<a href="#fnref4" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Jeffrey Dean, Greg S. Corrado, Rajat Monga, KaiChen,
Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew
Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed
deep networks. In NIPS, 2012.<a href="#fnref5" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep
learning. Book in preparation for MIT Press, 2016.<a href="#fnref6"
class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal covariate shift.
In ICML, 2015.<a href="#fnref7" class="footnote-back"
role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://xinyudu.cn">小杜老公</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://xinyudu.cn/2023/11/30/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-01/">http://xinyudu.cn/2023/11/30/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://xinyudu.cn" target="_blank">喜欢小杜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%AF%95%E4%B8%9A%E8%AE%BE%E8%AE%A1/">毕业设计</a><a class="post-meta__tags" href="/tags/%E5%B0%8F%E5%88%98%E7%9A%84/">小刘的</a><a class="post-meta__tags" href="/tags/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/">联邦学习</a></div><div class="post_share"><div class="social-share" data-image="/img/default_cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/11/29/%E7%AC%AC%E5%8D%81%E4%BA%94%E6%AC%A1%E8%AF%BE/" title="第十五次课-状态分配"><img class="cover" src="/img/default_cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">第十五次课-状态分配</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/head.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">小杜老公</div><div class="author-info__description">这是我和小杜的博客网站</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Liuwenhai666" target="_blank" title="Github"><i class="fab fa-github" style="color: #464E47;"></i></a><a class="social-icon" href="https://gitee.com/?from=osc-index" target="_blank" title="Gitee"><i class="fa-brands fa-gitter" style="color: #464E47;"></i></a><a class="social-icon" href="mailto:wenhai-liu@ncepu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #464E47;"></i></a><a class="social-icon" href="https://qm.qq.com/cgi-bin/qm/qr?k=3XIm_jkz9RxfLq2MfNNv0deYh492fu_n" target="_blank" title="QQ"><i class="fa-brands fa-qq" style="color: #464E47;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-01communication-efficient-learning-of-deep-networks-from-decentralized-data"><span class="toc-number">1.</span> <span class="toc-text">联邦学习入门-01(Communication-Efficient
Learning of Deep Networks from Decentralized Data)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E6%9C%89%E5%A6%82%E4%B8%8B%E7%9A%84%E5%B1%9E%E6%80%A7"><span class="toc-number">1.1.</span> <span class="toc-text">联邦学习：联邦学习有如下的属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%90%E7%A7%81"><span class="toc-number">1.2.</span> <span class="toc-text">隐私</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%94%E9%82%A6%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.</span> <span class="toc-text">联邦优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%94%E9%82%A6%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">联邦平均算法</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/11/30/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-01/" title="联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data)"><img src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data)"/></a><div class="content"><a class="title" href="/2023/11/30/%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-01/" title="联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data)">联邦学习入门-01(Communication-Efficient Learning of Deep Networks from Decentralized Data)</a><time datetime="2023-11-30T09:32:29.000Z" title="发表于 2023-11-30 17:32:29">2023-11-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/29/%E7%AC%AC%E5%8D%81%E4%BA%94%E6%AC%A1%E8%AF%BE/" title="第十五次课-状态分配"><img src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第十五次课-状态分配"/></a><div class="content"><a class="title" href="/2023/11/29/%E7%AC%AC%E5%8D%81%E4%BA%94%E6%AC%A1%E8%AF%BE/" title="第十五次课-状态分配">第十五次课-状态分配</a><time datetime="2023-11-29T04:22:55.000Z" title="发表于 2023-11-29 12:22:55">2023-11-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/27/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8D%81%E5%9B%9B%E5%91%A8%E5%91%A8%E4%B8%80%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/" title="数据结构-十四周周一最小生成树"><img src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构-十四周周一最小生成树"/></a><div class="content"><a class="title" href="/2023/11/27/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%8D%81%E5%9B%9B%E5%91%A8%E5%91%A8%E4%B8%80%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/" title="数据结构-十四周周一最小生成树">数据结构-十四周周一最小生成树</a><time datetime="2023-11-27T08:08:32.000Z" title="发表于 2023-11-27 16:08:32">2023-11-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%82%BB%E6%8E%A5%E8%A1%A8/" title="数据结构--邻接表"><img src="/2023/11/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%82%BB%E6%8E%A5%E8%A1%A8/c44e6da9cdb5a4b7c59e43115d87c96.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构--邻接表"/></a><div class="content"><a class="title" href="/2023/11/25/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E9%82%BB%E6%8E%A5%E8%A1%A8/" title="数据结构--邻接表">数据结构--邻接表</a><time datetime="2023-11-25T06:34:43.000Z" title="发表于 2023-11-25 14:34:43">2023-11-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/24/%E7%AC%AC%E5%8D%81%E5%9B%9B%E6%AC%A1%E8%AF%BE/" title="第十四次课-同步时序逻辑电路化简状态"><img src="/img/default_cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第十四次课-同步时序逻辑电路化简状态"/></a><div class="content"><a class="title" href="/2023/11/24/%E7%AC%AC%E5%8D%81%E5%9B%9B%E6%AC%A1%E8%AF%BE/" title="第十四次课-同步时序逻辑电路化简状态">第十四次课-同步时序逻辑电路化简状态</a><time datetime="2023-11-24T12:16:29.000Z" title="发表于 2023-11-24 20:16:29">2023-11-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By 小杜老公</div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn"><span>冀ICP备2023034833号</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="我爱你,I love you,愛してます,Ti Amo" data-fontsize="22px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>